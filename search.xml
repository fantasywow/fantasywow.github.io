<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2019闲书阅读书目]]></title>
    <url>%2Fblog%2F2019-reading-list%2F</url>
    <content type="text"><![CDATA[《扫地出门：美国城市的贫穷与暴利》《低欲望社会》《下流社会》《非对称风险》《深度学习 : 智能时代的核心驱动力量》《打开量化投资的黑箱》《反脆弱》《说谎者的扑克牌》《债务危机》《一本书看透股权架构》《贫穷的本质》《寻找Alpha：量化交易策略》《共同基金常识》《一部电影的诞生》]]></content>
  </entry>
  <entry>
    <title><![CDATA[CFA Level I exam 报名注册]]></title>
    <url>%2Fblog%2FCFA-level1-register%2F</url>
    <content type="text"><![CDATA[报名注册cfa的报名注册流程相对比较简单，在www.cfainstitute.org上用邮箱注册用户，然后就可以www.cfainstitute.org/programs/cfa/register里注册考试。 需要注意的主要有: 确定报名考试的时间，一年两次，6月一次，12月一次，其中Level II,III只能在6月考。 报名以及考试需要护照。 费用CFA的考试报名费用跟报名时间相关，报名越早越便宜。而且不同年份还可能不一样，选实体教材还要额外加钱。 以2020 June为例： 费用包含两部分 Program enrollment fee: USD 450 （只用交一次） Exam Registration Fees：（每次考试都要交） 最后还有税费： 我是用全币种visa卡支付的。 电子资料之后可以在https://www.cfainstitute.org/en/programs/cfa/candidate上下载相关的电子资料]]></content>
      <tags>
        <tag>CFA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘比赛Handbook]]></title>
    <url>%2Fblog%2Fdata-mining-handbook%2F</url>
    <content type="text"><![CDATA[Pandas常用操作去重 1BASE_EXCG = BASE_EXCG.drop_duplicates(subset=None, keep='first', inplace=False) 合并 1IDV_TD = pd.merge(IDV_TD, BASE_EXCG, left_on='CCY_CD', right_on='CCY_LETE_CD', how='left') 删除 1IDV_TD.drop(['RAT_CTG','FXDI_SA_ACCM'], axis=1,inplace =True) 运算 1IDV_TD = IDV_TD.eval('CRBAL_RMB = CRBAL * RMB_MID_PRIC') 索引重置 1IDV_TD_after.reset_index() 修改列名 1IDV_TD_after_after.rename(columns= lambda x:x if x=='CUST_NO' else IDV_TD_'+x).to_csv("./data/IDV_TD_out.csv",index = False) 修改字段类型 1IDV_CUST_BASIC['OCP_CD']=IDV_CUST_BASIC['OCP_CD'].astype(str) 填充NaN 1IDV_CUST_BASIC['OCP_CD'].fillna('NULL',inplace=True) groupby+join 12IDV_TD_group = IDV_TD.groupby(['CUST_NO','DATA_DAT'])IDV_TD_after = IDV_TD_group.sum().join(IDV_TD_group.size().to_frame(name='count')).join(IDV_TD_group['CRBAL_RMB'].max(),rsuffix = 'max') 新建一列 1IDV_TD_after_after['HAS_IDV_TD'] = 1 常用编码方式One-hot12345#one-hot encodingn_columns = ['CCY_CD','RDEP_IND_CD','ACCT_STS_CD','DP_DAY_CD','RDEP_DP_DAY_CD']dummy_df = pd.get_dummies(IDV_TD[n_columns],columns=n_columns)IDV_TD = pd.concat([IDV_TD, dummy_df], axis=1)IDV_TD.drop(n_columns, axis=1,inplace =True) Label1234567#label encodingfrom sklearn.preprocessing import LabelEncoderle = LabelEncoder()DP_DAY_CD_lset = ["Y005","Y003","Y002","Y001","M006","M003","M001","D7","D1","#","NULL"]le.fit(DP_DAY_CD_lset)IDV_TD["DP_DAY_CD"] = le.transform(IDV_TD["DP_DAY_CD"])IDV_TD['RDEP_DP_DAY_CD'] = le.transform(IDV_TD['RDEP_DP_DAY_CD']) Mean encoding123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#mean encodingfrom sklearn.model_selection import KFold,StratifiedKFold class MeanEncoder:def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None): """ :param categorical_features: list of str, the name of the categorical columns to encode :param n_splits: the number of splits used in mean encoding :param target_type: str, 'regression' or 'classification' :param prior_weight_func: a function that takes in the number of observations, and outputs prior weight when a dict is passed, the default exponential decay function will be used: k: the number of observations needed for the posterior to be weighted equally as the prior f: larger f --&gt; smaller slope """ self.categorical_features = categorical_features self.n_splits = n_splits self.learned_stats = &#123;&#125; if target_type == 'classification': self.target_type = target_type self.target_values = [] else: self.target_type = 'regression' self.target_values = None if isinstance(prior_weight_func, dict): self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np)) elif callable(prior_weight_func): self.prior_weight_func = prior_weight_func else: self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))@staticmethoddef mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func): X_train = X_train[[variable]].copy() X_test = X_test[[variable]].copy() if target is not None: nf_name = '&#123;&#125;_pred_&#123;&#125;'.format(variable, target) X_train['pred_temp'] = (y_train == target).astype(int) # classification else: nf_name = '&#123;&#125;_pred'.format(variable) X_train['pred_temp'] = y_train # regression prior = X_train['pred_temp'].mean() col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg(&#123;'mean': 'mean', 'beta': 'size'&#125;) col_avg_y['beta'] = prior_weight_func(col_avg_y['beta']) col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean'] col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True) nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values return nf_train, nf_test, prior, col_avg_ydef fit_transform(self, X, y): """ :param X: pandas DataFrame, n_samples * n_features :param y: pandas Series or numpy array, n_samples :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features """ X_new = X.copy() if self.target_type == 'classification': skf = StratifiedKFold(self.n_splits) else: skf = KFold(self.n_splits) if self.target_type == 'classification': self.target_values = sorted(set(y)) self.learned_stats = &#123;'&#123;&#125;_pred_&#123;&#125;'.format(variable, target): [] for variable, target in itertools.product(self.categorical_features, self.target_values)&#125; for variable, target in itertools.product(self.categorical_features, self.target_values): nf_name = '&#123;&#125;_pred_&#123;&#125;'.format(variable, target) X_new.loc[:, nf_name] = np.nan for large_ind, small_ind in skf.split(y, y): nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine( X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func) X_new.iloc[small_ind, -1] = nf_small self.learned_stats[nf_name].append((prior, col_avg_y)) else: self.learned_stats = &#123;'&#123;&#125;_pred'.format(variable): [] for variable in self.categorical_features&#125; for variable in self.categorical_features: nf_name = '&#123;&#125;_pred'.format(variable) X_new.loc[:, nf_name] = np.nan for large_ind, small_ind in skf.split(y, y): nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine( X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func) X_new.iloc[small_ind, -1] = nf_small self.learned_stats[nf_name].append((prior, col_avg_y)) return X_newdef transform(self, X): """ :param X: pandas DataFrame, n_samples * n_features :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features """ X_new = X.copy() if self.target_type == 'classification': for variable, target in itertools.product(self.categorical_features, self.target_values[:-1]): nf_name = '&#123;&#125;_pred_&#123;&#125;'.format(variable, target) X_new[nf_name] = 0 for prior, col_avg_y in self.learned_stats[nf_name]: X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name] X_new[nf_name] /= self.n_splits else: for variable in self.categorical_features: nf_name = '&#123;&#125;_pred'.format(variable) X_new[nf_name] = 0 for prior, col_avg_y in self.learned_stats[nf_name]: X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name] X_new[nf_name] /= self.n_splits return X_new 使用: 12345#mean-encodingMeanEncodeFeature = ['PROV_CD','NATN_CD','CULT_DGR_CD','SPEC_TECH_PRFN_QUA_CD','OCP_CD','RES_CD']ME = MeanEncoder(MeanEncodeFeature)ME.fit_transform(IDV_CUST_BASIC_train,IDV_y)IDV_CUST_BASIC = ME.transform(IDV_CUST_BASIC) 模型训练集验证集划分12from sklearn.model_selection import train_test_splitX_dtrain ,X_deval,Y_dtrain,Y_deval = train_test_split(X_train,Y_train,test_size=0.3,random_state=1024,stratify=Y_train) LightGBM12345678910111213141516171819202122232425262728293031import lightgbm as lgb import pickle from sklearn.metrics import roc_auc_score lgb_train = lgb.Dataset(X_dtrain, Y_dtrain) lgb_eval = lgb.Dataset(X_deval, Y_deval) params = &#123; 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'num_leaves': 20, 'max_depth': 5, 'min_data_in_leaf': 20, 'learning_rate': 0.02, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'lambda_l1': 1, 'lambda_l2': 0.001, 'min_gain_to_split': 0.2, 'verbose': 5, 'is_unbalance': True, #重要 'random_state' : 1024&#125; gbm = lgb.train(params, lgb_train, num_boost_round=2000, valid_sets=lgb_eval, early_stopping_rounds=200) 导出特征重要性123456importance = gbm.feature_importance() names = gbm.feature_name() with open('./feature_importance.txt', 'w+') as file: for index, im in enumerate(importance): string = names[index] + ', ' + str(im) + '\n' file.write(string) 保存模型1gbm.save_model('./model.txt') KFOLD 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586from sklearn.model_selection import KFoldfrom sklearn.metrics import roc_auc_scorefrom sklearn.metrics import f1_scoreimport lightgbm as lgbimport gc# Extract feature namesfeature_names = list(X_train.columns)# Create the kfold objectk_fold = KFold(n_splits = 5, shuffle = True, random_state = 50) # Empty array for feature importancesfeature_importance_values = np.zeros(len(feature_names)) # Empty array for test predictionstarget_predictions = np.zeros(target.shape[0]) # Empty array for out of fold validation predictionsout_of_fold = np.zeros(X_train.shape[0])valid_f1 = []valid_threshold = []valid_auc = []# Iterate through each foldfor train_indices, valid_indices in k_fold.split(X_train): print('开始一次迭代') # Training data for the fold train_features, train_labels = X_train.iloc[train_indices], Y_train.iloc[train_indices] # Validation data for the fold valid_features, valid_labels = X_train.iloc[valid_indices], Y_train.iloc[valid_indices] lgb_train = lgb.Dataset(train_features, train_labels) lgb_eval = lgb.Dataset(valid_features, valid_labels) params = &#123; 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'num_leaves': 30, 'max_depth': 6, 'min_data_in_leaf': 20, 'learning_rate': 0.01, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'lambda_l1': 1, 'lambda_l2': 0.001, 'min_gain_to_split': 0.2, 'verbose': 5, 'is_unbalance': True, #重要 'random_state' : 1024 &#125; gbm = lgb.train(params, lgb_train, num_boost_round=2000, verbose_eval = False, valid_sets=lgb_eval, early_stopping_rounds=200) pred_train = gbm.predict(valid_features, num_iteration=gbm.best_iteration) pred_train = pd.DataFrame(pred_train) maxf1 = 0 maxi = 0 for i in np.arange(0.2,0.8,0.001): temp = pd.DataFrame(pred_train[0].apply(lambda x:1 if x&gt;i else 0)) score = f1_score(valid_labels,temp) if score &gt;maxf1 : maxf1 = score maxi = i valid_f1.append(maxf1) valid_threshold.append(maxi) # Record the best iteration best_iteration = gbm.best_iteration valid_auc.append(gbm.best_score['valid_0']['auc']) # Record the feature importances feature_importance_values += gbm.feature_importance() / k_fold.n_splits # Make predictions target_predictions += gbm.predict(target, num_iteration = best_iteration) / k_fold.n_splits # Record the out of fold predictions out_of_fold[valid_indices] = gbm.predict(valid_features, num_iteration = best_iteration) # Clean up memory gc.enable() del gbm, train_features, valid_features gc.collect()print(valid_f1,np.mean(valid_f1))print(valid_threshold,np.mean(valid_threshold))print(valid_auc,np.mean(valid_auc)) StackNet1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import lightgbm as lgb lightgbm = lgb.LGBMClassifier( boosting_type= 'gbdt', objective= 'binary', metric= 'auc', num_leaves= 20, max_depth= 5, min_data_in_leaf= 20, learning_rate= 0.02, feature_fraction= 0.9, bagging_fraction=0.9, bagging_freq= 5, lambda_l1=1, lambda_l2=0.001, min_gain_to_split= 0.2, verbose=5, is_unbalance=True, random_state=1024)import xgboost as xgbxgboost_gbtree = xgb.XGBClassifier( booster='gbtree', objective='binary:logistic', n_estimators = 500, learning_rate = 0.1, subsample= 0.9, colsample_bytree= 0.9, min_child_weight=2, max_depth= 7)from sklearn.model_selection import GridSearchCVfrom catboost import CatBoostClassifierparam_cb = &#123; 'learning_rate': 0.15, 'bagging_temperature': 0.1, 'l2_leaf_reg': 4, 'depth': 7, 'iterations' : 300, 'task_type':'CPU', 'loss_function' : "CrossEntropy", 'eval_metric' : "AUC", #'bootstrap_type' : 'Bayesian', #'random_seed':42, #'early_stopping_rounds' : 100,&#125;clf_ctb = CatBoostClassifier(**param_cb)models = [ ######## First level ######## [xgboost_gbtree, clf_ctb, lightgbm], ######## Second level ######## [lightgbm],]from pystacknet.pystacknet import StackNetClassifiermodel = StackNetClassifier( models, metric="auc", folds=3, restacking=True, use_retraining=True, use_proba=True, random_state=42, verbose=1,)model.fit(X_dtrain, Y_dtrain)]]></content>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018闲书阅读书目]]></title>
    <url>%2Fblog%2F2018-reading-list%2F</url>
    <content type="text"><![CDATA[《思维的发现》《为什么精英都是时间控》《止损：如何克服贪婪和恐惧》《黑天鹅 如何应对不可预知的未来（升级版）》《投资中不简单的事》《投资中最简单的事》《M型社会》《高频交易员：华尔街的速度游戏》《耶路撒冷三千年》《高盛小道消息》《哈佛·庆应超强逻辑谈判技巧》]]></content>
  </entry>
  <entry>
    <title><![CDATA[智能投顾]]></title>
    <url>%2Fblog%2FRobo-Advisor%2F</url>
    <content type="text"><![CDATA[构建智能投顾系统的一些感悟 背景1.马科维茨均值方差模型奠定了现代投资组合理论，后来人的各种模型也都没有脱离用风险和回报两个维度来衡量一种资产。2.在中国能够覆盖绝大多数种类的资产的就只有公募基金。3.通过公募基金组建投资组合又面临一个问题，是选用被动型的公募基金还是主动型公募基金，也就是选择投指数还是试图寻找更好的主动管理的公募基金来寻求超额收益。 智能投顾 VS FOF1.FOF更加标准化。2.智能投顾的持仓以及变动更加透明，调仓需要客户授权。3.智能投顾理想的千人千面看似很美好，个人感觉实际上却很难做到（大多数人的实际风险承受能力和他自己认为的承受能力并不匹配） 模型：1.最优比例-根据金融模型算出最优的资产配置比例，触发调仓的信号。2.基金池-如果选择主动型公募基金还涉及到是量化选基金还是依赖经验主义选基金。 交易系统：1.申购赎回调仓是一组交易，其中一只交易失败时的处理方案。2.考虑首次追加最小申购金额，最低留存，步长等公募基金限制（理想情况是选择那些限制可以忽略的公募基金，避免申购赎回调仓时的一系列衍生的交易金额限制）。3.分级的货币基金，升级降级时引发的各种问题。（如果智能投顾系统和原有的基金代销系统使用同一基金交易账号，就不可避免这面对这一坑爹问题）4.在不考虑垫资的情况下，调仓是一个漫长的过程。。。申购的金额、基金都是可能发生变化的，要有一套完整的调仓交易调度规则。5.基金暂停申购的处理，有短期的暂停申购（节假日前的货币基金），有长期的暂停申购，要分别处理。 先写这么多以后再补充]]></content>
  </entry>
  <entry>
    <title><![CDATA[2017闲书阅读书目]]></title>
    <url>%2Fblog%2F2017-reading-list%2F</url>
    <content type="text"><![CDATA[《星巴克-关于咖啡、商业和文化的传奇》《随机漫步的傻瓜》《像TED一样演讲》《红色资本-中国的非凡崛起与脆弱的金融基础》《斜杠青年-如何开启你的多重身份》《有效资产管理》《激荡三十年》《时运变迁》《硅谷钢铁侠》《助推》《信号》《腾讯传》]]></content>
  </entry>
  <entry>
    <title><![CDATA[JDK8中的effectively final]]></title>
    <url>%2Fblog%2Feffectively-final-JDK-8%2F</url>
    <content type="text"><![CDATA[However, starting in Java SE 8, a local class can access local variables and parameters of the enclosing block that are final or effectively final. A variable or parameter whose value is never changed after it is initialized is effectively final.https://docs.oracle.com/javase/tutorial/java/javaOO/localclasses.html 简单来说，如果只读一个外部变量，不再需要指定成final了。而在之前，常见的匿名内部类访问外部局部变量必须指定成final。 下面这段代码1.8就可以执行，1.7就会编译报错。 12345678910public void test()&#123; int aa = 10; final int bb = 20; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(aa+bb); &#125; &#125;);&#125; 1.7报错内容：Error:(22, 36) java: 从内部类中访问本地变量aa; 需要被声明为最终类型当你要修改的外部变量时，在1.8下也会直接报错： 1234567891011121314151617public void test()&#123; int aa = 10; final int bb = 20; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(aa+bb); &#125; &#125;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; aa = 20; //Error &#125; &#125;);&#125; 1.8报错内容Error:(34, 17) java: 从内部类引用的本地变量必须是最终变量或实际上的最终变量 (needs to be final or effectively final)]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Spring中使用Junit4测试时访问JNDI资源]]></title>
    <url>%2Fblog%2Fjunit-jndi-Spring%2F</url>
    <content type="text"><![CDATA[系统使用JNDI来访问在容器上配置的资源信息（例如数据库信息）,如果用xml来配置的话一般这么写 &lt;jee:jndi-lookup id=”dataSource” jndi-name=”jdbc/database” expected-type=”javax.sql.DataSource” /&gt; 但是使用Junit来做测试的时候并不会访问容器，也就无法获取资源信息。这个时候你可以另外写一份applicationContextTest.xml来供测试用，或者可以在Junit里配置JNDI资源。 12345678910111213141516171819202122232425262728@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = "classpath:applicationContext.xml")public class XXXXXXTest &#123; @BeforeClass public static void setUpJndi() throws IllegalStateException, NamingException&#123; //配置DataSource SybDataSource ds = new SybDataSource(); ds.setServerName("127.0.0.1"); ds.setPortNumber(5000); ds.setDatabaseName("databaseName"); ds.setNetworkProtocol("Tds"); ds.setUser("user"); ds.setPassword("password"); ds.setCHARSET("cp936"); //配置ActiveMQ ActiveMQTopic topic = new ActiveMQTopic("testTopic"); ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory("tcp://localhost:61616"); //org.springframework.mock.jndi.SimpleNamingContextBuilder SimpleNamingContextBuilder builder = new SimpleNamingContextBuilder(); builder.bind("jdbc/database", ds); builder.bind("jms/topic", topic); builder.bind("jms/topicConnFactory", connectionFactory); builder.activate(); &#125; &#125; 实际使用的时候，直接使用生产的applicationContext很容易因为过于庞大而初始化过程缓慢，如果项目非常大还是应该单独写几个测试的applicationContext分模块来测试。]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>Junit4</tag>
        <tag>JNDI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring+Maven+H2（Jave配置,非Spring Boot）]]></title>
    <url>%2Fblog%2Fspring-h2%2F</url>
    <content type="text"><![CDATA[H2作为一款嵌入式数据库非常适合用来写一些Demo小程序之类的。 首先配置pom.xml添加H2的依赖。 12345&lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;version&gt;1.4.193&lt;/version&gt;&lt;/dependency&gt; 接下来在你的RootConfig里配置数据源 12345678910@Beanpublic DataSource dataSource() &#123; EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); EmbeddedDatabase db = builder .setType(EmbeddedDatabaseType.H2) .addScript("db/create-db.sql") .addScript("db/data-db.sql") .build(); return db;&#125; addScript的两个脚本是用来建表和添加测试数据的，期间发现org.h2.command.Parser解析create table语句必须写在一行里，不支持中间有换行符，按理说应该能如何配置下来支持。 想要开启H2的console还需要添加： 1234@Bean(initMethod="start",destroyMethod="stop")public org.h2.tools.Server h2WebConsonleServer () throws SQLException &#123; return org.h2.tools.Server.createWebServer("-web","-webAllowOthers","-webDaemon","-webPort", "8888");&#125; 之后就可以通过localhost:8888访问H2的控制台了 够简单。]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>H2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[升级hexo至3.2.2]]></title>
    <url>%2Fblog%2Fupdate-hexo-to-3-2-2%2F</url>
    <content type="text"><![CDATA[因为Win10的自动更新把系统彻底搞崩了，重装之后需要恢复hexo环境，正好借机将原来的2.X版本升级到最新版3.2.2，记录下过程以及中间遇到的问题： nodejs&amp;&amp;git没什么好说的从官网下载最新版安装即可 安装hexonpm install -g hexo-cli 新版的hexo把很多部分拆出来模块化了，还需要单独安装，主要需要后安装server和git部署模块npm install hexo-server --savenpm install hexo-deployer-git --save 文件迁移我选择新init了一个目录来做迁移，init之后比照着修改_config.yml，之后将原来的source_posts下的md文件全部复制过来，以及themes\metro-light经过我细微修改的metro-light主题目录 生成然后hexo g生成测试下，报错了。。 INFO Start processing FATAL Something&apos;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html Template render error: (unknown path) [Line 7, Column 23] Error: Unable to call `the return value of (posts[&quot;first&quot;])[&quot;updated&quot;][&quot;toISOString&quot;]`, which is undefined or falsey at Object.exports.prettifyError (D:\OneDrive\blog\node_modules\hexo-generator-feed\node_modules\nunjucks\src\lib.js:34:15) at D:\OneDrive\blog\node_modules\hexo-generator-feed\node_modules\nunjucks\src\environment.js:486:31 at root [as rootRenderFunc] (eval at &lt;anonymous&gt; (D:\OneDrive\blog\node_modules\hexo-generator-feed\node_modules\nunjucks\src\environment.js:565:24), &lt;anonymous&gt;:161:3) at Obj.extend.render (D:\OneDrive\blog\node_modules\hexo-generator-feed\node_modules\nunjucks\src\environment.js:479:15) at Hexo.module.exports (D:\OneDrive\blog\node_modules\hexo-generator-feed\lib\generator.js:28:22) at Hexo.tryCatcher (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\util.js:16:23) at Hexo.&lt;anonymous&gt; (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\method.js:15:34) at D:\OneDrive\blog\node_modules\hexo\lib\hexo\index.js:337:24 at tryCatcher (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\util.js:16:23) at MappingPromiseArray._promiseFulfilled (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\map.js:61:38) at MappingPromiseArray.PromiseArray._iterate (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\promise_array.js:113:31) at MappingPromiseArray.init (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\promise_array.js:77:10) at MappingPromiseArray._asyncInit (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\map.js:30:10) at Async._drainQueue (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\async.js:143:12) at Async._drainQueues (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\async.js:148:10) at Immediate.Async.drainQueues [as _onImmediate] (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\async.js:17:14) at processImmediate [as _immediateCallback] (timers.js:383:17) FATAL (unknown path) [Line 7, Column 23] Error: Unable to call `the return value of (posts[&quot;first&quot;])[&quot;updated&quot;][&quot;toISOString&quot;]`, which is undefined or falsey Template render error: (unknown path) [Line 7, Column 23] Error: Unable to call `the return value of (posts[&quot;first&quot;])[&quot;updated&quot;][&quot;toISOString&quot;]`, which is undefined or falsey at Object.exports.prettifyError (D:\OneDrive\blog\node_modules\hexo-generator-feed\node_modules\nunjucks\src\lib.js:34:15) at D:\OneDrive\blog\node_modules\hexo-generator-feed\node_modules\nunjucks\src\environment.js:486:31 at root [as rootRenderFunc] (eval at &lt;anonymous&gt; (D:\OneDrive\blog\node_modules\hexo-generator-feed\node_modules\nunjucks\src\environment.js:565:24), &lt;anonymous&gt;:161:3) at Obj.extend.render (D:\OneDrive\blog\node_modules\hexo-generator-feed\node_modules\nunjucks\src\environment.js:479:15) at Hexo.module.exports (D:\OneDrive\blog\node_modules\hexo-generator-feed\lib\generator.js:28:22) at Hexo.tryCatcher (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\util.js:16:23) at Hexo.&lt;anonymous&gt; (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\method.js:15:34) at D:\OneDrive\blog\node_modules\hexo\lib\hexo\index.js:337:24 at tryCatcher (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\util.js:16:23) at MappingPromiseArray._promiseFulfilled (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\map.js:61:38) at MappingPromiseArray.PromiseArray._iterate (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\promise_array.js:113:31) at MappingPromiseArray.init (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\promise_array.js:77:10) at MappingPromiseArray._asyncInit (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\map.js:30:10) at Async._drainQueue (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\async.js:143:12) at Async._drainQueues (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\async.js:148:10) at Immediate.Async.drainQueues [as _onImmediate] (D:\OneDrive\blog\node_modules\hexo\node_modules\bluebird\js\release\async.js:17:14) at processImmediate [as _immediateCallback] (timers.js:383:17)看了眼似乎是hexo-generator-feed插件的问题_config.yml： plugins: ##- hexo-generator-feed注释掉这个插件后能够顺利生成了看这个插件的Github上的issue目测是还没支持3.2.2（后面发现不是这个问题）https://github.com/hexojs/hexo-generator-feed/issues/43P.S.新版本的生成过程中输出信息少了很多，一度以为有问题，new了个新文章发现也是这样。 本地测试之后准备运行server来本地先看看效果。结果hexo server说没有这个命令。。顿时迷茫了。。尝试重装了hexo和hexo-server依然不行，各种研究之后看到一个说是因为配置文件里的plugins项，看了眼hexo 3.0的改动日志。 Plugin Loading Mechanism In Hexo 3, only plugins listed in package.json will be loaded, and plugins/scripts will be executed in sandbox so hexo namespace will be no longer exposed to global.这样看来_config.yml中的plugins项就失去了意义直接删除之后就可以顺利运行，连上面注释掉的feed插件也正常了。比照了下网站上的内容，发现没什么问题，进入下一步部署相关内容。 部署先在Git Bash里生成ssh密钥ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot;将公钥配置到Github和Coding.net中（我是同时部署到这两个网站，在dnspod里配置DNS信息，联通和电信访问Coding.net其他就访问Github）再ssh -T git@github.comssh -T git@git.coding.net测试下连通性，第一次的话一般会提示: The authenticity of host &apos;github.com (192.30.253.113)&apos; can&apos;t be established. RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &apos;github.com,192.30.253.113&apos; (RSA) to the list of known hosts. Hi XXXXXXX! You&apos;ve successfully authenticated, but GitHub does not provide shell access.将网站的公钥加入到known_hosts中。]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql中join on和join using的异同]]></title>
    <url>%2Fblog%2Fsql-join-on-using%2F</url>
    <content type="text"><![CDATA[使用join on和join using的目的都是通过某个列连接在一起，但是有以下不同点： on可以通过表达式来连接不同名称的列，例如on (tableA.columnA = tableB.columnB),而using必须是同名列。 返回的结果集不一样：看下图：可以看到用on的时候相当于把两个表的连接那列都返回回来，而using只返回了一列，并且这一列在select中是可以没有表名来限定的。P.S.查资料看有些数据库(oracle?)的话这一列是不能有表名限定的，在我本地的MySQL是都可以的。]]></content>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache CXF部署到WebSphere Application Server]]></title>
    <url>%2Fblog%2FApacheCXF_on_WebSphere%2F</url>
    <content type="text"><![CDATA[在开发过程中CXF是部署的本地的Tomcat上的，开发完并用SoapUI简单测试过之后开始准备部署到测试环境上，测试环境是使用的WebSphere Application Server 8.0.0.5版本。部署上去后访问wsdl页面报error500，JVMVRFY013错误，头一次看到JVM错误。。。 首先看了ApacheCXF官网的文章：application-server-specific-configuration-guide.html#ApplicationServerSpecificConfigurationGuide-ForWebSphere6.1.0.29+,V7andV8以及IBM的文档：1001_thaker.pdf。 1.参照里面说的确认Class loader设置成了Classes loaded with local class loader first(parent last)。2.设置JVM参数-Dcom.ibm.websphere.webservices.DisableIBMJAXWSEngine=true（或者修改war包里的WebContent/META-INF/MANIFEST.MF添加DisableIBMJAXWSEngine:true）来禁用了IBM JAX-WS implementation。 修改完之后依然same error…于是乎想着更换下CXF的版本从2.3.9换成2.7.18（因为要注册到ESB上，一开始给我们的推荐版本就是2.3.9没敢直接换到3.X.X），突然间，奇迹发生了，wsdl页面能正常显示出来了，可惜高兴没多久就发现调用服务的时候依然会报错same error。。。。 最后。。再一次。。被Stackoverflow拯救了 then deleting from our deploy the following jar: activation-*, stax-api-* (but not stax2-api!), jaxb-api-*, jaxb-impl-*, xercesImpl-*, xml-apis-*发现项目中包含了activation.1.1.1.jar(其实并不知道这个jar包是干什么的。。),删了之后发现一切正常了！]]></content>
      <tags>
        <tag>Web Service</tag>
        <tag>CXF</tag>
        <tag>WebSphere</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记解决引入CXF的jar包导致异常情况的过程及所学知识]]></title>
    <url>%2Fblog%2FApache-CXF-override-Provider%2F</url>
    <content type="text"><![CDATA[为了在一个已有系统中实现对外提供Web Service，并在ESB中注册，使用Apache CXF来实现提供服务的功能。在项目中引入了CXF及其依赖的JAR包并修改web.xml和spring的applicationcontext.xml（CXF和Spring集成的真好）之后，发现原有的调用其它Web Service返回的结果中文出现乱码，并且报文头出现了一些区别。逐步排查发现只要引入JAR包就会出现这个问题，于是乎开始深入研究调用Web Service调用过程来定位问题所在。 调用Web Service服务需要WSDL文件以及根据这个WSDL文件生成的jar包,jar包里除了对外暴露的接口类之外，还有一个继承javax.xml.ws.Service的类和一个包含所有可调用方法的接口类。在使用时通过Class.forName(Service的子类)，并通过getConstructor(URL.class, Qname.class).newInstance(XX,XX)来实例化一个Service类，再通过getPort来获取接口的代理类,进而再调用具体的方法。 在对比分析之后，发现引入CXF包的前后，Service类中的delegate的具体实现类发生了变化。开始怀疑是因为引入的CXF的JAR包通过某种方式覆盖了默认的实现类。 最后还是借助了伟大的面向Stackoverflow编程大法:JAX-WS = When Apache CXF is installed it “steals” default JDK JAX-WS implementation 引用下大牛答案:Apache CXF (cxf-rt-frontend-jaxws-\*.jar to be precise) registers itself as a JAX-WS provider in the JVM.Inside the aforementioned JAR there is a file named: /META-INF/services/javax.xml.ws.spi.Provider with the following contents:org.apache.cxf.jaxws.spi.ProviderImplIf you now look at javax.xml.ws.spi.FactoryFinder#find method you will discover that JDK searches the CLASSPATH for the presence of javax.xml.ws.spi.Provider file and falls back to default Sun implementation if not available. So you have two options to force fallback: either remove cxf-rt-frontend-jaxws-\*.jar from CLASSPATH or override javax.xml.ws.spi.Provider file provided by CXF to point to fallback location The second option is actually a bit easier. Simply create:/src/main/resources/META-INF/services/javax.xml.ws.spi.Providerfile (assuming you are using Maven) with the following contents:org.apache.cxf.jaxws.spi.ProviderImplThat’s it, tested with javax.xml.ws.Endpoint#publish. 在CXF的jar包中有这么个文件/META-INF/services/javax.xml.ws.spi.Provider(文件名就是javax.xml.ws.spi.Provider)，内容只有一行org.apache.cxf.jaxws.spi.ProviderImpl。这个时候就涉及到了JAVA SPI(Service Provider Interface)技术，简单来说就是通过配置文件来动态指定实现类。这篇博客写的非常浅显易懂:Java SPI机制简介 在删除了CXF的JAR包中的javax.xml.ws.spi.Provider文件之后就一切恢复了正常，但是这并没有很好的解决问题，删除之后使用默认的Provider对CXF的功能有什么影响还有待测试，但起码定位了问题，后续继续研究学习！]]></content>
      <tags>
        <tag>Web Service</tag>
        <tag>CXF</tag>
        <tag>JAVA</tag>
        <tag>SPI(Service Provider Interface)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加权随机算法]]></title>
    <url>%2Fblog%2FWeighted%20Random%20Distribution%2F</url>
    <content type="text"><![CDATA[加权随机算法一般应用在以下场景：有一个集合S，里面比如有A,B,C,D这四项。这时我们想随机从中抽取一项，但是抽取的概率不同，比如我们希望抽到A的概率是50%,抽到B和C的概率是20%,D的概率是10%。一般来说，我们可以给各项附一个权重，抽取的概率正比于这个权重。那么上述集合就成了： {A:5，B:2，C:2，D:1} ###方法一： 扩展这个集合，使每一项出现的次数与其权重正相关。在上述例子这个集合扩展成：{A,A,A,A,A,B,B,C,C,D}然后就可以用均匀随机算法来从中选取。 好处：选取的时间复杂度为O（1）,算法简单。坏处：空间占用极大。另外如果权重数字位数较大，例如{A:49.1 B：50.9}的时候，就会产生巨大的空间浪费。 ###方法二： 计算权重总和sum，然后在1到sum之间随机选择一个数R，之后遍历整个集合，统计遍历的项的权重之和，如果大于等于R，就停止遍历，选择遇到的项。 还是以上面的集合为例，sum等于10，如果随机到1-5，则会在遍历第一个数字的时候就退出遍历。符合所选取的概率。 好处：没有额外的空间占用，算法也比较简单。坏处：选取的时候要遍历集合，时间复杂度是O（n）。 ###方法三： 可以对方法二进行优化，对项目集按照权重排序。这样遍历的时候，概率高的项可以很快遇到，减少遍历的项。比较{A:5，B:2，C:2，D:1}和{B:2，C:2，A:5，D:1}前者遍历步数的期望是5/10*1+2/10*2+2/10*3+1/10*4而后者是2/10*1+2/10*2+5/10*3+1/10*4。 好处：提高了平均选取速度。坏处：需要进行排序，并且不易添加删除修改项。]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人理财-基金]]></title>
    <url>%2Fblog%2Ffinance-fund%2F</url>
    <content type="text"><![CDATA[马上就要走向工作了，趁着还有时间，研究一下个人理财，为将来的资产增值做好准备！（其实就是工资太低。。。）P.S.可能有不准确的地方。 #基金 一些基本的名词解释： 一级市场：就是公司发行股票，对个人来说一般申购新股（打新）也就是在一级市场买股票。二级市场：也就是在交易所中买卖，也就是个人和个人之间交易股票。（个人也泛指机构一类）场内场外：场内就是只交易所内，买卖需要在证券公司开户，场外则是可以通过银行或者在线购买产品。未知价格交易法：场外的基金申购和赎回时都是不确定净值的，当日15点之前的交易按照当天的净值计算，15点之后的按照第二天的净值计算。基金净值更新时间: ETF基金净值是随时更新 LOF基金,每天更新5次.9:30 10:30 11:30 2:00 3:00点的净值是晚上出来 普通开放基金每天一次(一般是晚上18点以后) 封闭基金每周更新一次 ##股票基金 股票基金顾名思义就是以投资股票为主，根据选择股票的方向可以按行业，主题等划分。 ##债券基金 债券基金是以投资债券为主，普通债券基金风险很低，但是仍然是会亏损的。 ##可转换债券 可转换债券是债券的一种，利率低但是可以转换成公司股票，当股市很好的时候很多可转换债券基金收益比股票基金的收益都高。 ##分级基金 分级基金一般来说是将母基金分成子基金A和子基金B，一般子基金A会获得一个比较稳定的收益，当母基金获益时，满足A的稳定收益之后，其他收益都归B所有，也就是收益增加B会获得更高的收益，而当母基金亏损时，会满足A的收益，加大B的亏损。这样通过杠杆，使两只子基金一只低风险低收益，一只高风险高收益。 ##ETF 交易型开放式指数基金，通常又被称为交易所交易基金（Exchange Traded Funds，简称“ETF”）ETF的最大特点是在场内交易，但有一些场外基金与ETF进行联接。A股有两只300ETF:510300,159919。 ##LOF LOF基金，英文全称是”Listed Open-Ended Fund”，汉语称为”上市型开放式基金”。LOF是在场内场外都能进行交易的基金，一些分级基金在运作期满之后会转换成LOF基金。]]></content>
      <tags>
        <tag>理财</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双基准快速排序 Dual-Pivot Quicksort]]></title>
    <url>%2Fblog%2FDual-Pivot_Quicksort%2F</url>
    <content type="text"><![CDATA[课本上常见的快速排序都是选择一个枢纽元（Pivot），基于这个枢纽元从前后双向扫描分成大于枢纽元和小于枢纽元的。而从JDK 7开始，java.util.Arrays.sort()使用双基准快速排序（Dual-Pivot Quicksort）作为实现。 —update JDK7使用双基准快速排序来排序基本元素，针对对象用TimSort 123456789public static void sort(int[] a) &#123; DualPivotQuicksort.sort(a, 0, a.length - 1, null, 0, 0); &#125;public static void sort(Object[] a) &#123; if (LegacyMergeSort.userRequested) //java -Djava.util.Arrays.useLegacyMergeSort=true 这个是运行时配置的 legacyMergeSort(a); else ComparableTimSort.sort(a, 0, a.length, null, 0, 0);&#125; 传统快速排序： 选择枢纽元pivot,有很多种选法，但都只有一个。 基于枢纽元分成大于和小于的两部分，并且枢纽元放到最终的位置。 递归排序大于和小于的两部分。 双基准快速排序： 对于长度小于17的数组使用插入排序(常见优化步骤，传统快排也有应用)。 选择两个枢纽元p1,p2，一般选择起始元素a[left]和末尾元素a[right]（有其他选取方式）。 假设p1&lt;p2，如果不是就交换。 基于这p1,p2将整个数组分成三部分，&lt;p1的,p1&lt;&amp;&lt;p2的,&gt;p2的。 递归排序这三个部分。 其中4这个步骤是采用单向扫描，leetcode上的Sort Colors这题一般就是采用同样的方法分成三部分。https://oj.leetcode.com/problems/sort-colors/ 详细的可以参考下面论文，里面有详细的细节以及算法实现。 ###Reference： DualPivotQuicksort.pdf]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[限定ASP.NET Web API返回对象的Content-Type为application/json]]></title>
    <url>%2Fblog%2Ffix_content-type_to_Applicationjson_in_WebApi%2F</url>
    <content type="text"><![CDATA[ASP.NET WebAPI是一套RESTful API开发框架。会自动序列化返回的对象成XML或JSON,其序列化后的格式取决于Request的header中的Accept。常见的有： application/json application/xml text/json text/xml(这两种已经被废弃了） 如果不指定的话默认返回的Content-Type是：application/json; charset=utf-8 所以使用Chrome和IE11来Get Web Api返回的格式就不同，Chrome是application/xml，而IE11是application/json，就因为两个浏览器请求的Header中的Accept字段不一样。Chrome：Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8IE11:Accept:text/html, application/xhtml+xml, */*区别就在于Chrome多了一个application/xml。 ###想要指定格式，有两种方法：方法一是放弃自动序列化，选择返回HttpResponseMessage，再手动设置 12HttpResponseMessage resp = new HttpResponseMessage();resp.Content = new StringContent(jsoncontent, System.Text.Encoding.UTF8, &quot;application/json&quot;); 但是这样就失去了自动序列化带来的便捷，以及导致生成Web API Help Page无法捕获返回的对象。（这也是我为什么要研究这个的原因。。。。结果后来发现可以用[ResponseType(typeof(List&lt;PushPin&gt;))]来指定实际返回的对象） 方法二就是设置HttpConfiguration，设置成只支持application/json。 12345678910111213141516public static class WebApiConfig&#123; public static void Register() &#123; // Use this class to set configuration options for your mobile service ConfigOptions options = new ConfigOptions(); // Use this class to set WebAPI configuration options HttpConfiguration config = ServiceConfig.Initialize(new ConfigBuilder(options)); config.Formatters.XmlFormatter.SupportedMediaTypes.Clear(); var appjsonType = config.Formatters.JsonFormatter.SupportedMediaTypes.FirstOrDefault(t =&gt; t.MediaType == &quot;text/json&quot;); config.Formatters.JsonFormatter.SupportedMediaTypes.Remove(appjsonType); // To display errors in the browser during development, uncomment the following // line. Comment it out again when you deploy your service for production use. // config.IncludeErrorDetailPolicy = IncludeErrorDetailPolicy.Always; &#125;&#125; 关键是这几句： 12345//删除所有的xml格式，也就是text/xml,application/xmlconfig.Formatters.XmlFormatter.SupportedMediaTypes.Clear(); //删除text/json var appjsonType=config.Formatters.JsonFormatter.SupportedMediaTypes.FirstOrDefault(t =&gt; t.MediaType == &quot;text/json&quot;);config.Formatters.JsonFormatter.SupportedMediaTypes.Remove(appjsonType); 其实就是使Web API不支持其他格式了。但是这种方法有个弊端，是全局的，但是应该可以限定范围，暂时先不管了。 P.S.吐槽下，IE默认是不显示JSON的，会直接下载下来，所以开发人员工具就捕获不到HTTP报文，也没法看Header。找了个方法解决之： 为了测试更方便，一般我使用 Web API 都会设置让 Web API 返回 Json 格式。 在IE浏览器中，当伙同在地址输入 URL 后 IE 浏览器会弹出是否需要下载的提示。 实际上 IE 弹出下载提示也没有什么不好的，但是有时候想要迫不及待的看到Json返回的结果时，又需要反复的下载再打开查看，这个行为就显得让人讨厌了。 如果能够像 Chrome 或 Firefox 可直接看结果就好了。 解决办法也非常简单，需要我们在操作系统的注册表中添加关于 JSON 的 MIME 类型支持，你可以将以下内容编辑成扩展名为 .reg 的文件（文件名随意），然后双击执行将该文件导入注册表： Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\MIME\Database\Content Type\application/json] &quot;CLSID&quot;=&quot;{25336920-03F9-11cf-8FD0-00AA00686F13}&quot; &quot;Encoding&quot;=dword:00080000 [HKEY_CLASSES_ROOT\MIME\Database\Content Type\text/json] &quot;CLSID&quot;=&quot;{25336920-03F9-11cf-8FD0-00AA00686F13}&quot; &quot;encoding&quot;=dword:00080000 其原理是修改注册表，将 application/json、text/json 两种 Content-Type 开启设置调成与 GIF/PNG/HTML 一致，改为直接用浏览器打开查看。 导入上面的注册表文件以后，再使用IE开启就可以不需要下载即可显示 json 了。From http://www.iefans.net/ie-dakai-json-xianshi/]]></content>
      <tags>
        <tag>Web API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Consistent Hashing一致性哈希库libconhash的使用与实现]]></title>
    <url>%2Fblog%2FConsistent_Hashing_libconhash%2F</url>
    <content type="text"><![CDATA[闲着没事看了看一致性哈希，找了个开源库libconhash看看如何实现。整过过程非常清晰，代码也非常易懂，注释也非常全，带sample，简直就是开源库的典范！！libconhash的sourceforge下载地址 From wikipedia 一致哈希 是一种特殊的哈希算法。在使用一致哈希算法后，哈希表槽位数（大小）的改变平均只需要对K/n 个关键字重新映射，其中 K是关键字的数量，n是槽位数量。然而在传统的哈希表中，添加或删除一个槽位的几乎需要对所有关键字进行重新映射。需求:在使用n台缓存服务器时，一种常用的负载均衡方式是，对资源o的请求使用hash(o) = o mod n来映射到某一台缓存服务器。当增加或减少一台缓存服务器时这种方式可能会改变所有资源对应的hash值，也就是所有的缓存都失效了，这会使得缓存服务器大量集中地向原始内容服务器更新缓存。因些需要一致哈希算法来避免这样的问题。 一致哈希尽可能使同一个资源映射到同一台缓存服务器。这种方式要求增加一台缓存服务器时，新的服务器尽量分担存储其他所有服务器的缓存资源。减少一台缓存服务器时，其他所有服务器也可以尽量分担存储它的缓存资源。 一致哈希算法的主要思想是将每个缓存服务器与一个或多个哈希值域区间关联起来，其中区间边界通过计算缓存服务器对应的哈希值来决定。（定义区间的哈希函数不一定和计算缓存服务器哈希值的函数相同，但是两个函数的返回值的范围需要匹配。）如果一个缓存服务器被移除，则它会从对应的区间会被并入到邻近的区间，其他的缓存服务器不需要任何改变。实现:一致哈希将每个对象映射到圆环边上的一个点，系统再将可用的节点机器映射到圆环的不同位置。查找某个对象对应的机器时，需要用一致哈希算法计算得到对象对应圆环边上位置，沿着圆环边上查找直到遇到某个节点机器，这台机器即为对象应该保存的位置。 当删除一台节点机器时，这台机器上保存的所有对象都要移动到下一台机器。添加一台机器到圆环边上某个点时，这个点的下一台机器需要将这个节点前对应的对象移动到新机器上。 更改对象在节点机器上的分布可以通过调整节点机器的位置来实现。 #使用方式首先进行初始化 1struct conhash_s *conhash = conhash_init(NULL); 其中参数是个函数指针，指定将字符串转成long int的hash函数。默认hash函数的实现是: 1234567891011121314151617long __conhash_hash_def(const char *instr)&#123; int i; long hash = 0; unsigned char digest[16]; conhash_md5_digest((const u_char*)instr, digest); /* use successive 4-bytes from hash as numbers */ for(i = 0; i &lt; 4; i++) &#123; hash += ((long)(digest[i*4 + 3]&amp;0xFF) &lt;&lt; 24) | ((long)(digest[i*4 + 2]&amp;0xFF) &lt;&lt; 16) | ((long)(digest[i*4 + 1]&amp;0xFF) &lt;&lt; 8) | ((long)(digest[i*4 + 0]&amp;0xFF)); &#125; return hash;&#125; 1struct node_s g_nodes[64]; node_s是标识节点的结构体，定义如下： 123456struct node_s&#123; char iden[64]; /* node name or some thing identifies the node */ u_int replicas; /* number of replica virtual nodes */ u_int flag;&#125;; 接下来是设置节点以及将添加节点： 1234/* set nodes */conhash_set_node(&amp;g_nodes[0], &quot;titanic&quot;, 32);/* add nodes */conhash_add_node(conhash, &amp;g_nodes[0]); 其中titanic是这个节点的唯一标识符也就是node_s中的iden，一般可以用机器名或IP地址。而后面的32则代表这个节点对应的虚拟节点的数量replicas，一般来说一个节点的虚拟节点数占总的虚拟节点数的比重越大，那么分配到这个节点的条目也就越多。flag是标识节点状态的，有 12#define NODE_FLAG_INIT 0x01 /* node is initialized */#define NODE_FLAG_IN 0x02 /* node is added in the server */ 两种状态 删除节点与查询： 12conhash_del_node(conhash, &amp;g_nodes[0]);struct node_s node = conhash_lookup(conhash, str); 释放资源： 1conhash_fini(conhash); #内部实现原理 libconhash使用红黑树来保存虚拟节点。当一个节点插入后，生成所有的虚拟节点标识符字符串，并进行hash运算转成long之后插入到红黑树。 1234567891011121314151617181920//replicas是节点的虚拟节点数量for(i = 0; i &lt; node-&gt;replicas; i++)&#123; //生成虚拟几点的标识符 __conhash_node2string(node, i, buff, &amp;len); //计算标识符的hash，cb_hashfunc是初始化时制定的hash函数指针 hash = conhash-&gt;cb_hashfunc(buff); //如果红黑树种没有存在的话就插入其中。 if(util_rbtree_search(&amp;(conhash-&gt;vnode_tree), hash) == NULL) &#123; //生成红黑树节点 rbnode = __conhash_get_rbnode(node, hash); if(rbnode != NULL) &#123; //生成成功就插入到红黑树中 util_rbtree_insert(&amp;(conhash-&gt;vnode_tree), rbnode); conhash-&gt;ivnodes++; &#125; &#125;&#125; 其中生成红黑树节点的代码： 12345678910111213141516171819202122util_rbtree_node_t *__conhash_get_rbnode(struct node_s *node, long hash)&#123; util_rbtree_node_t *rbnode; rbnode = (util_rbtree_node_t *)malloc(sizeof(util_rbtree_node_t)); if(rbnode != NULL) &#123; rbnode-&gt;key = hash; rbnode-&gt;data = malloc(sizeof(struct virtual_node_s)); if(rbnode-&gt;data != NULL) &#123; struct virtual_node_s *vnode = rbnode-&gt;data; vnode-&gt;hash = hash; vnode-&gt;node = node; &#125; else &#123; free(rbnode); rbnode = NULL; &#125; &#125; return rbnode;&#125; virtual_node_s保存了hash值和实际节点的指针： 12345struct virtual_node_s&#123; long hash; struct node_s *node; /* pointer to node */&#125;; 而删除节点就是将红黑树中的它的所有虚拟节点删除其中虚拟节点标识符的生成规则是： 1_snprintf_s(buf, 127, _TRUNCATE, &quot;%s-%03d&quot;, node-&gt;iden, replica_idx); 而查找的流程就是计算查找对象的hash然后查找红黑树中比他大的最小节点如果没有比他大的就选择最小的节点。 1234567hash = conhash-&gt;cb_hashfunc(object);rbnode = util_rbtree_lookup(&amp;(conhash-&gt;vnode_tree), hash);if(rbnode != NULL)&#123; struct virtual_node_s *vnode = rbnode-&gt;data; return vnode-&gt;node;&#125; 红黑树的查找代码： 1234567891011121314151617181920if((rbtree != NULL) &amp;&amp; !util_rbtree_isempty(rbtree))&#123; util_rbtree_node_t *node = NULL; util_rbtree_node_t *temp = rbtree-&gt;root; util_rbtree_node_t *null = _NULL(rbtree); while(temp != null) &#123; if(key &lt;= temp-&gt;key) &#123; node = temp; /* update node */ temp = temp-&gt;left; &#125; else if(key &gt; temp-&gt;key) &#123; temp = temp-&gt;right; &#125; &#125; /* if node==NULL return the minimum node */ return ((node != NULL) ? node : util_rbtree_min(rbtree));&#125;]]></content>
      <tags>
        <tag>Consistent Hashing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java实现多继承(转载)]]></title>
    <url>%2Fblog%2Fjava_implement_muilt_inheritance%2F</url>
    <content type="text"><![CDATA[多重继承指的是一个类可以同时从多于一个的父类那里继承行为和特征，然而我们知道Java为了保证数据安全，它只允许单继承。有些时候我们会认为如果系统中需要使用多重继承往往都是糟糕的设计,这个时候我们往往需要思考的不是怎么使用多重继承,而是您的设计是否存在问题.但有时候我们确实是需要实现多重继承，而且现实生活中也真正地存在这样的情况，比如遗传：我们即继承了父亲的行为和特征也继承了母亲的行为和特征。可幸的是Java是非常和善和理解我们的,它提供了两种方式让我们曲折来实现多重继承：接口和内部类。 #一、 接口 在介绍接口和抽象类的时候了解到子类只能继承一个父类，也就是说只能存在单一继承，但是却可以实现多个接口，这就为我们实现多重继承做了铺垫。 对于接口而已，有时候它所表现的不仅仅只是一个更纯粹的抽象类，接口是没有任何具体实现的，也就是说，没有任何与接口相关的存储，因此也就无法阻止多个接口的组合了。 12345678910111213141516171819202122232425262728293031interface CanFight &#123; void fight();&#125;interface CanSwim &#123; void swim();&#125;interface CanFly &#123; void fly();&#125;public class ActionCharacter &#123; public void fight()&#123; &#125;&#125;public class Hero extends ActionCharacter implements CanFight,CanFly,CanSwim&#123; public void fly() &#123; &#125; public void swim() &#123; &#125; /** * 对于fight()方法，继承父类的，所以不需要显示声明 */&#125; #二、内部类 上面使用接口实现多重继承是一种比较可行和普遍的方式，在介绍内部类的时候谈到内部类使的多继承的实现变得更加完美了，同时也明确了如果父类为抽象类或者具体类，那么我就仅能通过内部类来实现多重继承了。如何利用内部类实现多重继承，请看下面实例：儿子是如何利用多重继承来继承父亲和母亲的优良基因。 首先是父亲Father和母亲Mother： 1234567891011public class Father &#123; public int strong()&#123; return 9; &#125;&#125;public class Mother &#123; public int kind()&#123; return 8; &#125;&#125; 重头戏在这里，儿子类Son： 12345678910111213141516171819202122232425public class Son &#123; /** * 内部类继承Father类 */ class Father_1 extends Father&#123; public int strong()&#123; return super.strong() + 1; &#125; &#125; class Mother_1 extends Mother&#123; public int kind()&#123; return super.kind() - 2; &#125; &#125; public int getStrong()&#123; return new Father_1().strong(); &#125; public int getKind()&#123; return new Mother_1().kind(); &#125;&#125; 儿子继承了父亲，变得比父亲更加强壮，同时也继承了母亲，只不过温柔指数下降了。这里定义了两个内部类，他们分别继承父亲Father类、母亲类Mother类，且都可以非常自然地获取各自父类的行为，这是内部类一个重要的特性：内部类可以继承一个与外部类无关的类，保证了内部类的独立性，正是基于这一点，多重继承才会成为可能。]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[访问HubSection中的ListView（windows 8）]]></title>
    <url>%2Fblog%2FAccess_controls_in_hubsection%2F</url>
    <content type="text"><![CDATA[首先一条特别重要的是：在HubSection中的控件是无法在代码中直接访问的。 &lt;HubSection Width=&quot;800&quot; x:Uid=&quot;Section1Header&quot; Header=&quot;Header&quot;&gt; &lt;DataTemplate&gt; &lt;ListView x:Name=&quot;listview&quot;&gt; &lt;ListView.ItemTemplate&gt; &lt;DataTemplate&gt; &lt;StackPanel Orientation=&quot;Horizontal&quot; &gt; &lt;TextBox Text=&quot;{Binding text}&quot;/&gt; &lt;/StackPanel&gt; &lt;/DataTemplate&gt; &lt;/ListView.ItemTemplate&gt; &lt;/ListView&gt; &lt;/DataTemplate&gt; &lt;/HubSection&gt;这样一个ListView如果你像平常一样在代码里 listView.ItemsSource = someObservableCollection;会报错The name ‘listView’ does not exist in the current context 一个非常简单的解决办法是在ListView里添加Loaded事件函数。 &lt;ListView x:Name=&quot;listview&quot; Loaded=&quot;ListView_Loaded&quot;&gt;然后添加函数 private void NotificationList_Loaded(object sender, RoutedEventArgs e) { var listView = (ListView)sender; listView.ItemsSource = someObservableCollection; }总感觉应该有更好的方法。]]></content>
      <tags>
        <tag>windows8开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向Windows8应用推送通知(Toast)并响应点击通知事件]]></title>
    <url>%2Fblog%2Fwindows8_app_post_toast_and_launch_browser%2F</url>
    <content type="text"><![CDATA[在Mobile Service服务器端推送Toast WindowsPushMessage message = new WindowsPushMessage(); message.XmlPayload = @&quot;&lt;?xml version=&quot;&quot;1.0&quot;&quot; encoding=&quot;&quot;utf-8&quot;&quot;?&gt;&quot; + @&quot;&lt;toast launch=&quot;&quot;toastTest&quot;&quot;&gt;&quot;+ //launch后面的是传输给Onlaunch的参数 @&quot;&lt;visual&gt;&lt;binding template=&quot;&quot;ToastText01&quot;&quot;&gt;&quot; + @&quot;&lt;text id=&quot;&quot;1&quot;&quot;&gt;&quot; + item.Text+&quot;/&quot;+item.UserId+ @&quot;&lt;/text&gt;&quot; + @&quot;&lt;/binding&gt;&lt;/visual&gt;&lt;/toast&gt;&quot;; try { var result = await Services.Push.SendAsync(message); Services.Log.Info(result.State.ToString()); } catch (System.Exception ex) { Services.Log.Error(ex.Message, null, &quot;Push.SendAsync Error&quot;); }当你点击Toast时会触发OnLaunched函数，参数内容可以在toast里面指定。这样就可以在OnLaunched根据参数做出指定的操作。 protected override void OnLaunched(LaunchActivatedEventArgs e) { string launchString = e.Arguments; if (launchString.Equals(&quot;toastTest&quot;)) { TestToastLaunchBrowers(); } ………… }这里我们通过点击Toast来启动一个外部浏览器并打开一个URL：使用Windows.System.Launcher.LaunchUriAsync(Uri)该方法可以在windows8以及wp8中使用 async void TestToastLaunchBrowers() { // Launch the URI var success = await Windows.System.Launcher.LaunchUriAsync(new Uri(&quot;http://www.bing.com&quot;)); }参考文献：How to handle activation from a toast notification (XAML)]]></content>
      <tags>
        <tag>windows8开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GZIP is not enough!网站压缩讲座总结]]></title>
    <url>%2Fblog%2Fgzip_is_not_enough%2F</url>
    <content type="text"><![CDATA[看了一个关于网站压缩的google视频讲座，总结下。 讲座地址：GZIP is not enough! (YouTube..需翻墙)PPT：PPT 虽然一个网站占主要大小的是图片，但HTML,JS,CSS的大小也是在不断增加的，所以文本相关的压缩是十分重要的。 推广了一下google的图片格式webp，既可以想PNG一样透明，也可以像JPEG一样有损压缩，甚至可以像GIF一样包含动画。 随着各种新技术的诞生，一些JS的体积十分巨大，压缩是必不可少的。 讲了讲Minification和Compression的区别，前者仅仅是通过减少可读性来缩小体积，而后者则是重新编码。 GZIP原理：LZ77（字典编码的一种）+HUFFMAN（哈夫曼编码） 几种压缩算法：GZIP,LZMA,LPAQ,BZIP2互有优劣，但是后面讲了因为整个互联网上可以说只有GZIP是普及了，使用其他压缩算法可能会带来问题。 进行预处理，如[0,1,2,3,4,5,6,7,8,9]用GZIP进行压缩体积不会缩小甚至会变大，但是进行delta编码成[0,1,1,1,1,1,1,1,1]转换成差值，重复元素就会很多，压缩也就会很有效率。 GZIP会使一些小文件膨胀。 PNG的压缩算法和GZIP是一个原理，图片的细微差距会导致压缩体积的巨大区别。 可以自己使用7ZIP或Zopfli进行压缩而不是让系统自己压缩，会有大约10%+的优势。 网站经常使用JSON来传输数据，通过变换JSON（Transpose JSON）进而缩小体积，例如： { “name”: “alex”, “pos”: “AUS”, }, { “name”: “Colt”, “pos”: “USA”, } 转换成： { “name”: [“alex”, “colt”], “pos”: [“AUS”, “USA”], }不光本身体积小了，也有利于压缩。 通过Dense Codes算法预处理来优化压缩（Compression boosting）。测试结果Dense Codes+7-Zip效果最好 使用Delta.js来传输文件新旧版本之间的的差值（Delta compression）。 文件间的水平差值，例如需要传输三个文件file1,file2,file3可以传输fil1,file2-file1,file3-file2。 最后结尾：GZIP is not enoughIt’s up to you to control your data 个人一句话总结：基本来说没有万能方法，挨个试一遍选最小的方案吧。。。]]></content>
      <tags>
        <tag>web</tag>
        <tag>GZIP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm package.json中的dependencies和devDependencies的区别]]></title>
    <url>%2Fblog%2Fdifference_between_dependencies_and_devdependencies_in_npm%2F</url>
    <content type="text"><![CDATA[一个node package有两种依赖，一种是dependencies一种是devDependencies，其中前者依赖的项该是正常运行该包时所需要的依赖项，而后者则是开发的时候需要的依赖项，像一些进行单元测试之类的包。 如果你将包下载下来在包的根目录里运行 npm install默认会安装两种依赖，如果你只是单纯的使用这个包而不需要进行一些改动测试之类的，可以使用 npm install --production只安装dependencies而不安装devDependencies。 如果你是通过以下命令进行安装 npm install packagename那么只会安装dependencies，如果想要安装devDependencies，需要输入 npm install packagename --dev 参考文献：npm官方文档：package.jsonnpm-install]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git自动转换换行符(autocrlf)带来的问题]]></title>
    <url>%2Fblog%2Fgit-auto-crlf-problem%2F</url>
    <content type="text"><![CDATA[最近在使用Github时遇到个问题。众所周知，在各操作系统下，文本文件所使用的换行符是不一样的。UNIX/Linux 使用的是 0x0A（LF），早期的 Mac OS 使用的是0x0D（CR），后来的 OS X 在更换内核后与 UNIX 保持一致了。但 DOS/Windows 一直使用 0x0D0A（CRLF）作为换行符。 Git提供了一个换行符自动转换功能试图解决这个问题–autocrlf，他包含了三种状态,true,input,false，详细内容可以看下面。Windows平台上的默认设置是true,这样在你签出代码的时候他会自动转换成你操作系统所使用CRLF，然后在提交的时候又自动转换成LF。 这个时候问题来了，autocrlf在Mac下默认设置成false,因为现在Mac也是用LF，按理说就没必要进行任何转换，但是如果你在windows操作系统上将版本库放在例如Dropbox这种云存储上，然后同步到Mac上，或者什么其他方式把windows文件引入到Mac下。那么在你提交的时候在Mac环境下不会自动将CRLF转换成LF，那么就会发现所有行都有改动。这个时候就需要设置成input。 Github上的建议配置，是在windows下将core.autocrlf设为true，在Mac和linux下设为input，这样就可以有效避免上述问题。 P.S.与换行符相关的命令 #AutoCRLF #提交时转换为LF，检出时转换为CRLF git config --global core.autocrlf true #提交时转换为LF，检出时不转换 git config --global core.autocrlf input #提交检出均不转换 git config --global core.autocrlf false #SafeCRLF #拒绝提交包含混合换行符的文件 git config --global core.safecrlf true #允许提交包含混合换行符的文件 git config --global core.safecrlf false #提交包含混合换行符的文件时给出警告 git config --global core.safecrlf warnP.S.S. git config 有三个作用域system，globe，local。优先级是local&gt;globe&gt;system。剩下的以后在研究 参考文章：7.1 自定义 Git - 配置 GitDealing with line endings]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[绑定自己的域名以及将DNS更换为DNSPOD]]></title>
    <url>%2Fblog%2Fchange-domain-name-and-name-server%2F</url>
    <content type="text"><![CDATA[首选你需要买一个域名，经过对比，个人感觉对于个人用户godaddy明显优于万网，最重要的是价格便宜！各种优惠，我通过google搜索godaddy的第一个广告链接进去发现一个.com的域名也才五十多，比用网上找的优惠卷都便宜，没想到google的广告还有这种福利。。。购买完域名后发现都说godaddy的DNS容易悲剧，于是要换成DNSPOD的。 首先参考Godaddy注册商域名修改DNS地址，进入godaddy的域名管理页面，将你的域名的nameserver改成： f1g1ns1.dnspod.net f1g1ns2.dnspod.net注意当你的nameserver改成DNSPOD后godaddy里面的DNS Zone file 里面的设置就没有意义了，你需要到DNSPOD上注册个账号在上面配置CNAME。 最后还需要在github上添加CNAME记录，你可以直接在hexo的source目录下添加个CNAME文件，文件内容是你的域名，也可以手动在github上你的博客repository里添加CNAME文件。]]></content>
      <tags>
        <tag>DNS</tag>
        <tag>DOMAIN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在iOS平台上的3D地图框架WhirlyGlobe上实现一个指南针]]></title>
    <url>%2Fblog%2Fimplement-compass-on-whirlyglobe%2F</url>
    <content type="text"><![CDATA[需要在WhirlyGlobe的3D地图上来实现一个指南针的功能，初步考虑需要以下几个步骤：监测地图位置，改变的时候触发事件——旋转指南针的图片到对应的角度。找到地图的缩放，拖动，旋转的方法后，发现所有涉及角度变换的地方都是改变了Eigen::Quaternionf rotQuat的值。下面介绍下Quaternion： Quaternion 的定义四元数一般定义如下： q=w+xi+yj+zk其中 w,x,y,z是实数。同时有: i*i=-1 j*j=-1 k*k=-1四元数也可以表示为： q=[w,v]其中v=(x,y,z)是矢量，w是标量，虽然v是矢量，但不能简单的理解为3D空间的矢量，它是4维空间中的的矢量，也是非常不容易想像的。 通俗的讲，一个四元数（Quaternion）描述了一个旋转轴和一个旋转角度。这个旋转轴和这个角度可以通过 Quaternion::ToAngleAxis转换得到。当然也可以随意指定一个角度一个旋转轴来构造一个Quaternion。这个角度是相对于单位四元数而言的，也可以说是相对于物体的初始方向而言的。 当用一个四元数乘以一个向量时，实际上就是让该向量围绕着这个四元数所描述的旋转轴，转动这个四元数所描述的角度而得到的向量。（摘自http://www.linuxgraphics.cn/opengl/opengl_quaternion.html） 简单来说Quaternion是用来表示旋转的，地图所存储的Quaternion变量表示了从初始位置到当前位置的旋转变量，这样我们可以通过计算北极点（0，0，1）经过旋转之后位置： Eigen::Vector3f northPole = (rot * Eigen::Vector3f(0,0,1)).normalized();P.S. Eigen是一个非常著名的C++数学库；然后就可以算出所需要的角度了： float angle = atan2(northPole.x(), northPole.y());//atan2为math.h下的C函数UIVIEW有一个transform属性来负责控制变型的，我们可以创建一个旋转变换来旋转图片，每次旋转当前的角度和上次的差值。 CGFloat rotation = 0.0 -(lastCompassAngle-angle); //lastCompassAngle存储了上次的角度 CGAffineTransform currentTransform = compass.transform;CGAffineTransform newTransform = CGAffineTransformRotate(currentTransform,rotation); [compass setTransform:newTransform];lastCompassAngle = angle;OK大功告成！！参考文章：http://www.cppblog.com/heath/archive/2009/12/13/103127.htmlhttp://www.linuxgraphics.cn/opengl/opengl_quaternion.html]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>WhirlyGlobe</tag>
        <tag>Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello hexo! 搭建hexo中参考的资料]]></title>
    <url>%2Fblog%2Freference-material-for-hexo%2F</url>
    <content type="text"><![CDATA[hexo官方文档官方文档总是最先参考的 hexo你的博客无意间看到这篇博客才突发奇想搞一个 hexo系列教程：（一）hexo介绍整个系列写的非常详细，有时间应该从头到尾看一遍 搭建过程中遇到个小问题，最开始通过https使用用户名和密码连接github，后来换成ssh后仍然需要用户名和密码，最后参考这篇文章解决了问题： 1、因为我们这里讲的是SSH连接方式，这也是我认为最可靠，最方便的一种连接方式，但有可能会出现每次push时，都需要输入用户名和密码，而我们用ssh -T git@github.com时，是没有问题的，这种情况多发生之前创建过HTTPS登陆方式，但信息没有清除干净，解决方法是，在git配置目录或hexo.deploy.git下，打开config文件，将HTTPS开头的字符串，改为it@github.com:账户名/账户名.github.com.git方式。]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
